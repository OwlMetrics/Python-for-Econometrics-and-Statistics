{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 5. Optimization and Root Finding \n",
    "Many estimation problems in econometrics and statistics are essentially optimization problems, which in turn are reduced to root finding (e.g., F.O.C. for smooth objective/criterion functions). The Scipy module contains a number of routines to find the extremum/roots of a user-supplied objective function located in \"scipy.optimize\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.optimize as opt                                     \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Root Finding\n",
    "### 5.0.1 Univariate Roots\n",
    "To find univariate roots of $f(x)$, `brentq()` is the recommended method, which is a combination of bisection, secant (safe version) and inverse quadratic interpolation. `brentq()` is a \"bracketed\" method which requires two initial values $a$, $b$ such that $f(a)f(b)<0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.9999999999999986, 2.999999999999924)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x**2 - 2*x -3\n",
    "\n",
    "opt.brentq(f, -2, 0), opt.brentq(f, 0, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Univariate roots of $f(x)$ can also be found using Newton-Rhapson method (if $f'$ is provided via parameter \"fprime\") and secent method (otherwise) by calling `newton()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0000000000000084, 3.0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.newton(f, -2), opt.newton(f, 4, fprime = lambda x: 2*x-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.0.2 Multivariate Roots\n",
    "Consider the task to solve a system of $m$ equations with $k$ unknowns. In practice, `root()` is preferred for polynomial equations and `fsolve()` is used for non-polynomial equations. Note that nonlinear system may have multiple solutions (or fixed points). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.] [ -2.64805212e-17   1.00000000e+00]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1.]), array([ -2.64805212e-17,   1.00000000e+00]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(z):\n",
    "    x, y = z[0], z[1]\n",
    "    return [x**2 - x*y + y - 1,\n",
    "            y**2 - x*y + x - 1]\n",
    "\n",
    "sol1, sol2 = opt.root(f, (0,0)), opt.root(f, (-1,1))\n",
    "print(sol1.x, sol2.x)\n",
    "opt.fsolve(f, (0,0)), opt.fsolve(f, (-1,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Unconstrained Optimization\n",
    "A typical optimization problem in econometrics and statistics has an objective/criterion function that returns the function value at a set of parameters for given data (e.g., a log-likelihood function). Scipy optimizers:\n",
    "1. require the objective function having the parameters as the first argument, and\n",
    "2. search the minimum of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Derivative-Based Methods\n",
    "This class of optimizers require derivative and/or Hessian information, and so they can be applied only to problems with (sufficiently) smooth objective functions.\n",
    "#### 5.1.1.1 BFGS (Broyden-Fletcher-Goldfarb-Shanno) Algorithm\n",
    "BFGS is a \"quasi\" Newton method of optimization, which requires only first derivative (\"first order method\"). The Hessian $H$ is replaced by some numerical approximation. BFGS should usually be the first optimizer explored for unconstrained problems. Analytical first derivative can be provided using the keyword `fprime`, otherwise a numerical approximation is used. In what follows, we can show the application of the BFGS optimizer `fmin_bfgs()` (and other optimizers) using the Logit model below:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "y_{i} & = & \\mathbf{1}[\\alpha_{0}+x_{i}'\\beta_{0}+\\epsilon_{i}>0]\n",
    "\\end{eqnarray*}\n",
    "\n",
    "with $\\epsilon_{i}\\sim logistic(0,1)$ and true parameters $\\alpha_{0}=0$,\n",
    "$\\beta_{0}=(1,1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 2723.343344\n",
      "         Iterations: 17\n",
      "         Function evaluations: 605\n",
      "         Gradient evaluations: 119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.01287609,  0.9858087 ,  0.99420628]),\n",
       " 2723.343344142441,\n",
       " array([  1.22070312e-04,   4.88281250e-04,  -3.05175781e-05]),\n",
       " array([[ 0.0008276 , -0.00044399,  0.00042738],\n",
       "        [-0.00044399,  0.00024138, -0.0002311 ],\n",
       "        [ 0.00042738, -0.0002311 ,  0.00022996]]),\n",
       " 605,\n",
       " 119,\n",
       " 2,\n",
       " [array([-0.79416259,  0.73143665, -0.17109954]),\n",
       "  array([-0.16186972,  0.85393667,  0.6069104 ]),\n",
       "  array([-0.20588848,  0.8401693 ,  0.64881369]),\n",
       "  array([-0.21515845,  0.86920038,  0.65390016]),\n",
       "  array([-0.18938   ,  0.87920006,  0.68900898]),\n",
       "  array([-0.13468649,  0.90148893,  0.76544025]),\n",
       "  array([-0.00599908,  0.96118179,  0.95180698]),\n",
       "  array([ 0.01158895,  0.98215448,  0.9893057 ]),\n",
       "  array([ 0.01290742,  0.98566319,  0.99407512]),\n",
       "  array([ 0.01288106,  0.9858053 ,  0.99420494]),\n",
       "  array([ 0.01287623,  0.98580871,  0.99420713]),\n",
       "  array([ 0.01287568,  0.98580886,  0.9942068 ]),\n",
       "  array([ 0.01287575,  0.98580898,  0.99420671]),\n",
       "  array([ 0.01287607,  0.98580867,  0.99420621]),\n",
       "  array([ 0.01287607,  0.98580868,  0.9942062 ]),\n",
       "  array([ 0.01287605,  0.98580872,  0.99420625]),\n",
       "  array([ 0.01287608,  0.98580871,  0.99420628]),\n",
       "  array([ 0.01287609,  0.9858087 ,  0.99420628])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simulate a Logit model\n",
    "a, b = 0, np.array([[1.0],[1]])\n",
    "np.random.seed(1) \n",
    "x, e = np.random.randn(5000,2), np.random.logistic(0,1,(5000,1))\n",
    "u = a + x@b + e\n",
    "y = 1*(u >= 0)\n",
    "data = np.hstack((y, x))\n",
    "\n",
    "# define the log-likelihood function \n",
    "def llh(params, hyperparams):\n",
    "    theta = params\n",
    "    data = hyperparams\n",
    "    a, b = theta[0], np.vstack((theta[1], theta[2]))\n",
    "    y, x = np.array(data[:,0:1]), np.array(data[:,1:data.shape[1]])\n",
    "    v = a + x@b\n",
    "    lh = y*np.log(np.exp(v)/(1+np.exp(v))) + (1-y)*np.log(1/(1+np.exp(v)))    \n",
    "    return (-1)*sum(lh)\n",
    "\n",
    "opt.fmin_bfgs(llh, np.random.randn(3,1),\n",
    "              args = (data,),\n",
    "              disp = True, maxiter = 1e6,\n",
    "              retall = True, full_output = True)\n",
    "\n",
    "# line 1: objective function, initial values\n",
    "# line 2: tuple containing hyperparams (i.e., data), use a \",\" to make it a tuple\n",
    "# line 3: display convergence message, maximum number of iterations\n",
    "# line 4: return results of each iteration, return function value, gradient, Hessian, warnflag, etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1.2 Conjugate Gradient Methods\n",
    "As an alternative to `fmin_bfgs()`, `fmin_cg()` uses a nonlinear conjugate gradient method to find the minimum. Analytical first derivative can be provided, otherwise it is numerically approximated. `fmin_ncg()` use a Newton conjugate gradient method and it requires a function which can compute the first derivative, though providing the Hessian is optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: 2723.343344\n",
      "         Iterations: 10\n",
      "         Function evaluations: 152\n",
      "         Gradient evaluations: 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.01287615,  0.98580896,  0.99420669])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.fmin_cg(llh, np.random.randn(3,1), args = (data,), disp = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Derivative-Free Optimization\n",
    "Derivative-free optimizers do not use derivative information and so can be used in a wider variety of problems such as\n",
    "functions which are not continuously differentiable. However, when applied to smooth objective functions, they are likely to be slower than derivative-based optimizers. An example for problems with an unsmooth objective function is the maximum score estimator for the Logit model which maximizes\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "Q_n(\\alpha,\\beta) & = & \\sum_{i=1}^{n}\\{y_{i}\\mathbf{1}[\\alpha+x_{i}'\\beta>0]+(1-y_{i})\\mathbf{1}[\\alpha+x_{i}'\\beta<0]\\}\n",
    "\\end{eqnarray*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2.1 Nelder-Mead Simplex\n",
    "While Newton's method is \"second order\" (i.e., requires the second derivative) and quasi-Newton methods are first order (requires only first derivatives), Nelder-Mead is a zero order method, i.e., it does not requires any derivatives and only uses the objective function itself. The Nelder-Mead simplex algorithm can be implemented using `fmin()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.726000\n",
      "         Iterations: 42\n",
      "         Function evaluations: 93\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.05367786,  0.92626561])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# objective function of maximum score estimation\n",
    "def maxscore(params, hyperparams):\n",
    "    theta = params\n",
    "    data = hyperparams\n",
    "    a, b = theta[0], np.vstack((theta[1], 1))\n",
    "    y, x = np.array(data[:,0:1]), np.array(data[:,1:data.shape[1]])\n",
    "    v = a + x@b\n",
    "    score = y*(v >= 0) + (1-y)*(v < 0)    \n",
    "    return (-1)*np.mean(score)\n",
    "\n",
    "np.random.seed(1)\n",
    "opt.fmin(maxscore, np.random.randn(2,1), args = (data,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2.2 Powell's Methods\n",
    "Powell's method is a derivative-free optimization method similar to conjugate-gradient, which can be implemented using `fmin_powell()`. In practice, `fmin_powell()` often converges faster (requires far fewer iterations) and is more reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -0.725200\n",
      "         Iterations: 3\n",
      "         Function evaluations: 142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.09593936,  0.97424993])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "opt.fmin_powell(maxscore, np.random.randn(2,1), args = (data,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Constrained Optimization\n",
    "In econometrics and statistics, constrained optimization is often used in hypothesis testing problems. The most common analytical technique is the method of Lagrange multipliers. `fmin_slsqp()` is the most general constrained optimizer using sequential least squares programming, which allows for equality, inequality and bounds constraints. To use it, constraints must be provided either as a list of callable functions or as a single function which returns an array. Functions which compute the derivative of the optimization target, the derivative of the equality constraints, and the derivative of the inequality constraints can be optionally provided. If not provided, these are numerically approximated. Note that for this optimizer,\n",
    "1. Arguments in constraints must be identical to those of the objective function.\n",
    "2. All constraints must take the form of $\\geq$ constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.9806773073765387\n",
      "            Iterations: 7\n",
      "            Function evaluations: 30\n",
      "            Gradient evaluations: 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.99717628,  0.49858814])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simulate a NLS model\n",
    "b = np.array([[1],[0.5]])\n",
    "x, e = np.random.randn(2000,1), np.random.randn(2000,1)\n",
    "y = np.exp(b[0] + b[1]*x) + e\n",
    "data = np.hstack((y, x))\n",
    "\n",
    "def nls(params, hyperparams):\n",
    "    b = params\n",
    "    data = hyperparams \n",
    "    y, x = np.array(data[:,0:1]), np.array(data[:,1:2])\n",
    "    dy = y - np.exp(b[0] + b[1]*x)       \n",
    "    return np.mean(dy**2)\n",
    "   \n",
    "def nls_constraints(params, hyperparams): \n",
    "    b = params\n",
    "    return np.array([b[1], b[0]-2*b[1], 2*b[1]-b[0]])\n",
    "\n",
    "# optimization with constraints\n",
    "opt.fmin_slsqp(nls, np.random.randn(2,1), args = (data,),\n",
    "               f_ieqcons = nls_constraints)                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.980668\n",
      "         Iterations: 12\n",
      "         Function evaluations: 72\n",
      "         Gradient evaluations: 18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.99827142,  0.49782124])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.fmin_bfgs(nls, np.random.randn(2,1), args = (data,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: All optimizers introduced here find only a _local_ minimum. To search for the _global_ minimum, users may try multiple (random) initial values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References for Optimization Algorithms\n",
    "1. \"Numerical Optimization\", by J. Nodecal and S. Wright, Springer, 2006.\n",
    "2. \"Convex Optimization\", by S. Boyd and L. Vandenberghe, Cambridge University Press, 2004."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
